"""
Defines the self-contained RAG (Retrieval-Augmented Generation) agent for the UW Procurement Assistant.

This module contains the `RAGAgent` class, which now includes all document processing logic.
It uses a LangGraph-powered workflow to process user queries, retrieve relevant information,
evaluate document sufficiency, and generate accurate, context-aware responses, including
offers to draft emails when direct answers are not available.
"""

import re
import asyncio
import json
import logging
import os
import uuid
from typing import Dict, Any, List, Optional, TypedDict
from datetime import datetime

from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

from .base_agent import BaseAgent, AgentState, AgentResponse
from ..services.azure_search_service import AdaptiveHybridAzureSearchRetriever
from ..services.memory_service import ConversationMemoryService

from ..services.query_processor import QueryProcessor
from langchain_core.output_parsers import JsonOutputParser
from ..config import settings


class RAGGraphState(TypedDict):
    """Represents the state of our RAG graph.
    
    Attributes:
        question: The user's question, potentially rewritten for better retrieval.
        original_question: The original, unmodified user question.
        generation: The final answer generated by the LLM.
        documents: The list of documents retrieved and processed through the pipeline.
        conversation_memory: The history of the conversation.
    """
    question: str
    original_question: str
    generation: str
    documents: List[Document]
    conversation_memory: dict


class RAGAgent(BaseAgent):
    """
    The primary agent for handling procurement-related queries using a RAG pipeline.

    This agent orchestrates a multi-step process using LangGraph to ensure high-quality
    responses. The process includes query rewriting, document retrieval, relevance grading,
    re-ranking, and final answer generation.
    """
    
    _prompt_template = """You are a professional, helpful, and highly-trained assistant for the University of Washington's procurement department.\n\n    CRITICAL INSTRUCTIONS:\n    1.  **Answer from Documents:** Base your answers *only* on the information found in the provided 'CONTEXT FROM KNOWLEDGE BASE' and 'CONVERSATION HISTORY'. Do not use external knowledge.\n    2.  **Procurement Focus:** Address questions about UW procurement, purchasing, policies, and processes. For non-procurement or general questions, politely state: \"I am designed to assist with UW procurement questions. How can I help you with a procurement-related topic?\".\n    3.  **Comprehensive Answers:** When the documents contain a full answer, provide a thorough and informative response. Use bullet points for clarity if needed.\n    4.  **Handling Incomplete Information:**\n        * If the documents do not contain the information to answer the question, but you find relevant 'CONTACT INFORMATION', you MUST respond with the following template:\n            `I could not find the specific information for your question, but I was able to find this contact who may be able to help: {contact_info}. I can help draft an email to this contact. Would you like me to proceed?`\n        * If the documents do not contain the answer AND no contact information is found, state: `I was unable to find an answer to your question in the available documents.`\n    5.  **Citations:** For every piece of information you provide from the knowledge base, you MUST include a numbered inline citation, like [1], [2], etc.\n    6.  **Professional Tone:** Maintain a professional, helpful, and human-like tone at all times.\n\n    ---\n    CONVERSATION HISTORY:\n    {history}\n\n    ---\n    CONTEXT FROM KNOWLEDGE BASE:\n    {context}\n\n    ---\n    CONTACT INFORMATION (only if found in documents):\n    {contact_info}\n\n    ---\n    QUESTION:\n    {question}\n\n    ---\n    Final Answer:\n    """

    def __init__(self):
        super().__init__("rag_agent", "RAG Agent")
        self.llm = None
        self.embeddings = None
        self.retriever = None
        self.memory_service = ConversationMemoryService("backend/conversation_memory.json")
        self.query_processor = None  # Will be initialized after LLM is set up
        self.graph = self._build_graph()
        
        # Setup structured logging for observability
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.logger.setLevel(logging.INFO)

        # Load prompt from external file
        self._enhanced_prompt_template = self._load_prompt_template()

    def _load_prompt_template(self) -> str:
        """Loads the prompt template from an external file."""
        prompt_path = os.path.join(os.path.dirname(__file__), "prompt_template.md")
        try:
            with open(prompt_path, "r") as f:
                return f.read()
        except FileNotFoundError:
            logging.error(f"Prompt template file not found at {prompt_path}")
            # Fallback to a default prompt if the file is missing
            return "You are a helpful assistant. Answer the user's question based on the context provided.\n\nContext: {context}\n\nQuestion: {question}"

    def _build_graph(self) -> StateGraph:
        """Builds and compiles the LangGraph workflow for the RAG pipeline."""
        self._initialize_models()
        
        workflow = StateGraph(RAGGraphState)

        # Define the nodes in the graph
        workflow.add_node("rewrite_query", self._rewrite_query_with_history)
        workflow.add_node("retrieve_documents", self._retrieve_documents)
        workflow.add_node("grade_documents", self._grade_documents_for_relevance)
        workflow.add_node("rerank_documents", self._rerank_documents_for_context)
        workflow.add_node("generate_answer", self._generate_answer)
        workflow.add_node("handle_no_documents", self._handle_no_documents_found)

        # Define the flow of the graph
        workflow.set_entry_point("rewrite_query")
        workflow.add_edge("rewrite_query", "retrieve_documents")
        workflow.add_edge("retrieve_documents", "grade_documents")
        workflow.add_edge("grade_documents", "rerank_documents")
        workflow.add_conditional_edges(
            "rerank_documents",
            self._decide_to_generate_or_fallback,
            {
                "generate": "generate_answer",
                "fallback": "handle_no_documents",
            },
        )
        workflow.add_edge("generate_answer", END)
        workflow.add_edge("handle_no_documents", END)

        return workflow.compile()

    async def process(self, state: AgentState) -> AgentResponse:
        """Implements the required abstract method from BaseAgent."""
        question = state.data.get("question", "")
        conversation_id = state.data.get("conversation_id", "default")
        
        try:
            self._initialize_models()
            initial_state = RAGGraphState(
                question=question,
                conversation_id=conversation_id,

                conversation_memory=self.memory_service.load_conversation_memory(conversation_id),
                documents=[],
                generation="",
                original_question=question
            )

            final_state = await self.graph.ainvoke(initial_state)
            self._update_conversation_memory(conversation_id, question, final_state)

            return AgentResponse(
                agent_id=self.agent_id,

                success=True,
                message=final_state.get("generation", ""),
                data={"sources": self._create_source_list(final_state.get("documents", []))}
            )
        except Exception as e:
            print(f"Error during RAG graph processing: {e}")
            return AgentResponse(
                agent_id=self.agent_id,

                success=False,
                message=f"An error occurred: {e}"
            )

    async def stream_run(self, question: str, conversation_id: Optional[str] = None):
        task_id = str(uuid.uuid4())
        """Executes the RAG graph as an async generator, yielding the final response."""
        # Structured logging for observability
        workflow_start = datetime.now()
        self._log_workflow_event("workflow_started", {
            "conversation_id": conversation_id,
            "question": question,
            "timestamp": workflow_start.isoformat()
        })
        
        try:
            self._initialize_models()
            
            # Load conversation memory and log context
            conversation_memory = self.memory_service.load_conversation_memory(conversation_id)
            self._log_workflow_event("memory_loaded", {
    
                "conversation_id": conversation_id,
                "memory_entries": len(conversation_memory.get("history", [])),
                "timestamp": datetime.now().isoformat()
            })
            
            initial_state = {
                "question": question,
                "original_question": question,
                "generation": "",
                "documents": [],
                "conversation_memory": conversation_memory,
            }

            final_state = None
            # Use astream to get the final state after all nodes have run
            async for event in self.graph.astream(initial_state):
                # Log each graph event for observability
                for node_name, node_output in event.items():
                    self._log_workflow_event("node_executed", {
            
                        "node_name": node_name,
                        "timestamp": datetime.now().isoformat()
                    })
                
                # Check for the final state in different possible formats
                if "__end__" in event:
                    final_state = event["__end__"]
                    self._log_workflow_event("workflow_completed", {
            
                        "has_generation": bool(final_state.get("generation")),
                        "document_count": len(final_state.get("documents", [])),
                        "timestamp": datetime.now().isoformat()
                    })
                    break
                # Also check for direct node outputs that might contain our generation
                for node_name, node_output in event.items():
                    if isinstance(node_output, dict) and "generation" in node_output:
                        final_state = node_output
            
            if final_state and final_state.get("generation"):
                generation = final_state.get("generation", "")
                
                # Log successful response generation
                self._log_workflow_event("response_generated", {
        
                    "response_length": len(generation),
                    "document_count": len(final_state.get("documents", [])),
                    "timestamp": datetime.now().isoformat()
                })
                
                try:
                    # Update memory with the final result
                    self._update_conversation_memory(conversation_id, question, final_state)
                    self._log_workflow_event("memory_updated", {
            
                        "conversation_id": conversation_id,
                        "timestamp": datetime.now().isoformat()
                    })
                except Exception as memory_error:
                    self._log_workflow_event("memory_error", {
            
                        "error": str(memory_error),
                        "timestamp": datetime.now().isoformat()
                    })
                
                # Calculate total processing time
                processing_time = (datetime.now() - workflow_start).total_seconds()
                self._log_workflow_event("workflow_finished", {
        
                    "processing_time_seconds": processing_time,
                    "success": True,
                    "timestamp": datetime.now().isoformat()
                })
                
                # Yield the final answer as a single chunk for the stream
                yield generation
            else:
                self._log_workflow_event("workflow_failed", {
        
                    "reason": "no_generation_found",
                    "timestamp": datetime.now().isoformat()
                })
                yield "I could not find an answer to your question."

        except Exception as e:
            self._log_workflow_event("workflow_error", {
    
                "error": str(e),
                "error_type": type(e).__name__,
                "timestamp": datetime.now().isoformat()
            })
            yield f"An error occurred: {e}"

    # --- Graph Node Implementations ---

    async def _rewrite_query_with_history(self, state: RAGGraphState) -> Dict[str, Any]:
        """Rewrites the user's query based on conversation history for better retrieval."""
        self.logger.info("---NODE: Rewriting query---")
        question = state["question"]
        conversation_memory = state["conversation_memory"]
        
        # Debug logging to understand conversation memory
        self.logger.info(f"DEBUG: Question = {question}")
        self.logger.info(f"DEBUG: Conversation memory type = {type(conversation_memory)}")
        self.logger.info(f"DEBUG: Conversation memory keys = {list(conversation_memory.keys()) if isinstance(conversation_memory, dict) else 'Not a dict'}")
        if isinstance(conversation_memory, dict) and "history" in conversation_memory:
            self.logger.info(f"DEBUG: History length = {len(conversation_memory['history'])}")
            if conversation_memory['history']:
                self.logger.info(f"DEBUG: First history entry = {conversation_memory['history'][0]}")
        
        rewritten_question = await self.query_processor.rewrite_query(question, conversation_memory)
        self.logger.info(f"DEBUG: Original question = {question}")
        self.logger.info(f"DEBUG: Rewritten question = {rewritten_question}")
        return {"question": rewritten_question, "original_question": question}

    def _retrieve_documents(self, state: RAGGraphState) -> Dict[str, Any]:
        """Retrieves documents from Azure Search based on the rewritten query."""
        self.logger.info("---NODE: Retrieving documents---")
        documents = self.retriever.invoke(state["question"])
        return {"documents": documents}

    def _grade_documents_for_relevance(self, state: RAGGraphState) -> Dict[str, Any]:
        """Grades retrieved documents for relevance to the original question."""
        self.logger.info("---NODE: Grading documents---")
        if not state.get("documents"):
            return {"documents": []}
        
        graded_docs = self._grade_documents_for_relevance_sync(state["documents"], state["original_question"])
        return {"documents": graded_docs}

    def _rerank_documents_for_context(self, state: RAGGraphState) -> Dict[str, Any]:
        """Re-ranks relevant documents to provide the best context for the answer."""
        self.logger.info("---NODE: Re-ranking documents---")
        if not state.get("documents"):
            return {"documents": []}
        
        reranked_docs = self._rerank_documents_for_context_sync(state["documents"], state["original_question"])
        return {"documents": reranked_docs}

    def _decide_to_generate_or_fallback(self, state: RAGGraphState) -> str:
        """Decides whether to generate an answer or use a fallback response."""
        self.logger.info("---DECISION: Checking for relevant documents---")
        if state.get("documents"):
            self.logger.info("---DECISION: Documents found, proceeding to generate.---")
            return "generate"
        self.logger.info("---DECISION: No relevant documents, using fallback.---")
        return "fallback"

    def _generate_answer(self, state: RAGGraphState) -> Dict[str, Any]:
        """Generates the final answer using the re-ranked documents and conversation history."""
        self.logger.info("---NODE: Generating answer---")
        documents = state.get("documents", [])
        original_question = state["original_question"]
        conversation_memory = state.get("conversation_memory", {})

        # Extract contacts and prepare context, regardless of document relevance for the main answer
        contacts = self._extract_contacts(documents)
        contact_info_str = self._prepare_contact_info_for_prompt(contacts)
        
        # Determine if the documents are sufficient for a direct answer
        is_sufficient = self._are_documents_sufficient_for_answer_sync(documents, original_question)

        # If documents are insufficient but a contact was found, offer to draft an email.
        if not is_sufficient and contact_info_str:
            self.logger.info("---LOGIC: Insufficient docs, but contact found. Offering email draft.---")
            generation = (
                f"I could not find the specific information for your question, but I was able to find this contact who may be able to help: "
                f"{contact_info_str}. I can help draft an email to this contact. Would you like me to proceed?"
            )
            return {"generation": generation, "documents": documents}

        # Proceed with enhanced RAG generation if documents are sufficient
        self.logger.info("---LOGIC: Sufficient docs found. Generating enhanced RAG answer.---")
        context = self._format_documents_for_context(documents)
        history = self.query_processor.format_conversation_history(conversation_memory.get("history", []))
        
        # Enhanced prompt template with better formatting
        enhanced_prompt = PromptTemplate(
            template=self._enhanced_prompt_template,
            input_variables=["context", "question", "history", "contact_info"],
        )
        rag_chain = enhanced_prompt | self.llm | StrOutputParser()
        
        generation = rag_chain.invoke({
            "context": context,
            "question": original_question,
            "history": history,
            "contact_info": contact_info_str,
        })
        
        return {"generation": generation, "documents": documents}

    def _handle_no_documents_found(self, state: RAGGraphState) -> Dict[str, Any]:
        """Handles cases where no relevant documents are found - implements enhanced email-offer logic."""
        self.logger.info("---NODE: Handling no documents found---")
        
        question = state.get("question", "")
        original_question = state.get("original_question", question)
        documents = state.get("documents", [])
        
        # Try to extract contacts from any documents that were retrieved (even if not relevant)
        contacts = self._extract_contacts(documents)
        
        # Create a more specific, visually appealing response
        if contacts:
            contact_info_str = self._prepare_contact_info_for_prompt(contacts)
            self.logger.info(f"---LOGIC: No relevant docs, but contact found: {contact_info_str}. Offering email draft.---")
            generation = self._create_specific_contact_response(original_question, contact_info_str)
        else:
            # No contacts found, provide helpful fallback with Richard Pallangyo as default contact
            self.logger.info("---LOGIC: No relevant docs and no contacts found. Providing enhanced guidance.---")
            generation = self._create_specific_fallback_response(original_question)
        
        return {
            "generation": generation,
            "documents": documents
        }
    
    def _create_specific_contact_response(self, question: str, contact_info: str) -> str:
        """Creates a specific, professional response when contacts are found."""
        return f"""**Procurement Information Search**

I searched our UW procurement knowledge base for information about "{question.lower()}", but I couldn't find specific details about this request.

**How I Can Help**
I found this relevant contact who may be able to assist you:

{contact_info}

Would you like me to help draft an email requesting information about your specific procurement needs?

**Related Information**
From our procurement policies, I know that specialized purchases often require additional documentation and approval processes."""
    
    def _create_specific_fallback_response(self, question: str) -> str:
        """Creates a specific, professional fallback response."""
        # Extract key terms from the question to make response more specific
        question_lower = question.lower()
        specific_terms = []
        
        # Identify specific procurement-related terms
        if "import" in question_lower or "customs" in question_lower:
            specific_terms.append("international procurement and customs requirements")
        if "equipment" in question_lower or "accelerator" in question_lower or "medical" in question_lower:
            specific_terms.append("specialized equipment procurement")
        if "switzerland" in question_lower or "canada" in question_lower or "germany" in question_lower:
            specific_terms.append("international vendor requirements")
        
        specific_context = ", ".join(specific_terms) if specific_terms else "specialized procurement questions"
        
        return f"""**Procurement Information Search**

I searched our UW procurement knowledge base for information about "{question.lower()}", but I couldn't find specific details about this specialized request.

**How I Can Help**
For {specific_context} like this, I recommend contacting:

**Richard Pallangyo** UW Procurement Office  
Email: rapaugustino@gmail.com

Would you like me to help draft an email to Richard requesting information about your specific procurement needs?

**What I Can Tell You**
From our procurement policies, I know that specialized purchases often require additional documentation, compliance verification, and may involve specific approval processes."""

    # --- Helper Methods ---

    def _initialize_models(self):
        """Initializes Azure OpenAI models and other essential services."""
        try:
            if self.llm is None:
                self.llm = AzureChatOpenAI(
                    azure_endpoint=settings.azure_openai_chat_endpoint,
                    api_key=settings.azure_openai_chat_key,
                    deployment_name=settings.azure_openai_chat_deployment,
                    api_version=settings.azure_openai_api_version,
                    temperature=0.1,
                )
            if self.embeddings is None:
                self.embeddings = AzureOpenAIEmbeddings(
                    azure_endpoint=settings.azure_openai_embedding_endpoint,
                    api_key=settings.azure_openai_embedding_key,
                    azure_deployment=settings.azure_openai_embedding_deployment,
                    api_version=settings.azure_openai_api_version,
                )
            if self.retriever is None:
                self.retriever = AdaptiveHybridAzureSearchRetriever(
                    search_service=settings.azure_search_service,
                    search_key=settings.azure_search_key,
                    index_name=settings.azure_search_index,
                    embeddings=self.embeddings
                )

            if self.query_processor is None:
                self.query_processor = QueryProcessor(self.llm)
        except Exception as e:
            # Consider more specific logging or error handling here
            print(f"Fatal error during model initialization: {e}")
            raise

    _grade_relevance_prompt_template = """You are a grader assessing the relevance of a retrieved document to a user question.\n\n    RULES:\n    - If the document contains keywords or semantic meaning relevant to the question, grade it as 'relevant'.\n    - If the document is not relevant, grade it as 'not relevant'.\n    - Your response must be a single JSON object with a single key 'score' and a value of either 'relevant' or 'not relevant'.\n\n    DOCUMENT:\n    {document}\n\n    QUESTION:\n    {question}\n\n    JSON RESPONSE:\n    """

    _grade_sufficiency_prompt_template = """You are a grader assessing if a set of documents contains enough information to fully answer a user's question.\n\n    RULES:\n    - Evaluate if the combined information in the documents is sufficient to provide a complete and direct answer.\n    - If yes, respond with a JSON object: {{"sufficient": true}}\n    - If no, respond with a JSON object: {{"sufficient": false}}\n\n    DOCUMENTS:\n    {documents}\n\n    QUESTION:\n    {question}\n\n    JSON RESPONSE:\n    """

    def _grade_documents_for_relevance_sync(self, documents: List[Document], question: str) -> List[Document]:
        """Grades documents for relevance and filters out irrelevant ones."""
        if not documents:
            return []

        prompt = PromptTemplate.from_template(self._grade_relevance_prompt_template)
        grading_chain = prompt | self.llm | JsonOutputParser()

        relevant_docs = []
        for doc in documents:
            try:
                result = grading_chain.invoke({"document": doc.page_content, "question": question})
                if result.get("score") == "relevant":
                    relevant_docs.append(doc)
            except Exception as e:
                print(f"Error grading document: {e}")
        return relevant_docs

    def _rerank_documents_for_context_sync(self, documents: List[Document], question: str) -> List[Document]:
        """Re-ranks relevant documents to place the most relevant one first (placeholder)."""
        return documents

    def _are_documents_sufficient_for_answer_sync(self, documents: List[Document], question: str) -> bool:
        """Checks if the provided documents are sufficient to answer the question."""
        if not documents:
            return False

        prompt = PromptTemplate.from_template(self._grade_sufficiency_prompt_template)
        sufficiency_chain = prompt | self.llm | JsonOutputParser()
        
        formatted_docs = self._format_documents_for_context(documents)

        try:
            result = sufficiency_chain.invoke({"documents": formatted_docs, "question": question})
            return result.get("sufficient", False)
        except Exception as e:
            print(f"Error checking document sufficiency: {e}")
            return False

    def _extract_contacts(self, documents: List[Document]) -> List[str]:
        """Extracts contact information (names, emails) from documents."""
        contacts = []
        email_pattern = r'[\w\.-]+@[\w\.-]+'
        for doc in documents:
            found_emails = re.findall(email_pattern, doc.page_content)
            for email in found_emails:
                name_search = re.search(f'([A-Z][a-z]+ [A-Z][a-z]+)(?=\s*\(?[<{email}>]?\)?)', doc.page_content)
                if name_search:
                    contacts.append(f"{name_search.group(1)} ({email})")
                else:
                    contacts.append(email)
        return list(set(contacts))

    def _prepare_contact_info_for_prompt(self, contacts: List[str]) -> str:
        """Formats the list of contacts into a string for the prompt."""
        if not contacts:
            return ""
        return "; ".join(contacts)

    def _format_documents_for_context(self, documents: List[Document]) -> str:
        """Formats documents into a single string for the RAG context."""
        context_str = ""
        for i, doc in enumerate(documents):
            context_str += f"Citation [{i+1}]:\n{doc.page_content}\n\n"
        return context_str

    def _create_source_list(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """Creates a list of source dictionaries from documents."""
        sources = []
        for i, doc in enumerate(documents):
            source_name = doc.metadata.get("source", f"Source {i+1}")
            sources.append({"name": source_name, "citation": i + 1})
        return sources

    def _log_workflow_event(self, event_type: str, event_data: Dict[str, Any]):
        """Logs structured workflow events for observability and dashboard integration."""
        log_entry = {
            "event_type": event_type,
            "agent_id": self.agent_id,
            "agent_name": self.name,
            **event_data
        }
        
        # Log as structured JSON for easy parsing by observability tools
        self.logger.info(f"WORKFLOW_EVENT: {json.dumps(log_entry)}")
        
        # Also print for immediate visibility during development
        print(f"[{event_type.upper()}] {json.dumps(event_data, indent=2)}")
    
    def _update_conversation_memory(self, conversation_id: str, question: str, final_state: RAGGraphState):
        """Updates the conversation memory with the latest interaction in a structured format for persistence."""
        # Create structured memory entry for future persistence
        memory_entry = {
            "question": question,
            "answer": final_state.get("generation", ""),
            "sources": self._create_source_list(final_state.get("documents", [])),
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "document_count": len(final_state.get("documents", [])),
                "response_length": len(final_state.get("generation", "")),
                "conversation_id": conversation_id
            }
        }
        
        # Log memory operation for observability
        self._log_workflow_event("memory_save_attempt", {
            "conversation_id": conversation_id,
            "entry_size": len(json.dumps(memory_entry)),
            "timestamp": datetime.now().isoformat()
        })
        
        # Save to current memory service (can be easily migrated to persistent storage)
        self.memory_service.save_conversation_memory(memory_entry, conversation_id)

    def _validate_input(self, state: AgentState) -> bool:
        """Validates the input state to ensure it contains a valid question."""
        question = state.data.get("question", "")
        return isinstance(question, str) and bool(question.strip())

    def get_capabilities(self) -> List[str]:
        """Returns a list of the agent's capabilities."""
        return [
            "procurement_qa",
            "policy_lookup",
            "contact_information"
        ]

        # Intelligent detection of when email assistance is genuinely needed
        def should_offer_email_assistance(question: str, documents: List[Document]) -> bool:
            """
            Determine if email assistance should be offered based on:
            1. Question complexity and specificity
            2. Document coverage of the topic
            3. Procurement relevance
            """
            question_lower = question.lower()
            doc_text = " ".join([doc.page_content.lower() for doc in documents])
            
            # Don't offer email for basic/general questions that are well-covered
            basic_questions = [
                "what are the core", "what are the main", "what are the basic",
                "contact information", "who is the contact", "how to contact",
                "who can i contact", "who should i contact", "who do i contact",
                "contact person", "point of contact", "whom should i contact",
                "what is the process", "how does the process work", "procurement process",
                "what is the procurement process", "how does procurement work",
                "what are the steps", "how do i", "where do i", "when do i",
                "what are the requirements", "what do i need", "how to"
            ]
            
            if any(basic in question_lower for basic in basic_questions):
                return False
            
            # Don't offer email for non-procurement questions
            non_procurement_indicators = [
                "weather", "time", "date", "location", "address", "phone",
                "personal", "health", "medical", "academic", "course", "class"
            ]
            
            if any(indicator in question_lower for indicator in non_procurement_indicators):
                return False
            
            # Only offer email for complex, specific procurement questions with insufficient info
            complex_procurement_indicators = [
                "specific requirements for", "detailed process for", "exact amount",
                "approval requirements", "documentation needed", "specialized",
                "experimental", "research equipment", "international", "over $",
                "threshold", "compliance", "regulatory"
            ]
            
            is_complex_question = any(indicator in question_lower for indicator in complex_procurement_indicators)
            
            # Check if documents provide insufficient detail for complex questions
            insufficient_detail_indicators = [
                "not specified", "not detailed", "not included", "not provided",
                "contact for more", "further information", "additional details",
                "more specific", "exact requirements", "detailed guidance"
            ]
            
            has_insufficient_detail = any(indicator in doc_text for indicator in insufficient_detail_indicators)
            
            # Only offer email for complex procurement questions with insufficient detail
            return is_complex_question and has_insufficient_detail
        
        # Format conversation history
        history = self.query_processor.format_conversation_history(
            conversation_memory.get("history", []), max_turns=3
        )
        
        prompt = PromptTemplate(
            template="""You are a professional, helpful, and highly-trained assistant for the University of Washington's procurement department.

        CRITICAL INSTRUCTIONS:
        1.  **Answer ONLY what was asked** - avoid general background information unless directly relevant.
        2.  **Be EXTREMELY CONCISE** - give the shortest possible answer that addresses the question.
        3.  **Citations:** Use numbered inline citations like [1], [2], [3] after each claim.
        4.  **Missing Information:** If documents don't contain the specific information requested, simply state: "I did not find [specific information] in the available documents."
        5.  **Email Assistance:** ONLY offer email assistance for PROCUREMENT-RELATED questions. Do NOT offer to email about general knowledge, geography, or non-procurement topics.
        6.  **No general policies** unless specifically asked about policies.
        7.  **No duplicate citations section** - sources will be listed automatically.

        ---
        CONVERSATION HISTORY:
        {history}
        ---
        SOURCE DOCUMENTS:
        {context}
        ---
        GUIDANCE FOR CLOSING:
        {contact_info}
        ---

        USER'S QUESTION: "{question}"

        YOUR CONCISE RESPONSE:
        """,
            input_variables=["question", "context", "history", "contact_info"],
        )

        rag_chain = prompt | self.llm | StrOutputParser()
        final_generation = rag_chain.invoke({
            "context": context, 
            "question": question, 
            "history": history,
            "contact_info": contact_info
        })
        
        # Append source list to the response
        if source_list:
            sources_section = "\n\n**📚 Sources:**\n" + "\n".join(source_list)
            final_generation_with_sources = final_generation + sources_section
        else:
            final_generation_with_sources = final_generation
        
        return {"generation": final_generation_with_sources}
    
    def _handle_no_docs(self, state: RAGGraphState) -> dict:
        """Handle case when no relevant documents found"""
        question = state["original_question"]
        
        # Use the same intelligent logic to determine if email assistance should be offered
        def should_offer_email_for_no_docs(question: str) -> bool:
            question_lower = question.lower()
            
            # Don't offer email for non-procurement questions
            non_procurement_indicators = [
                "weather", "time", "date", "location", "address", "phone",
                "personal", "health", "medical", "academic", "course", "class"
            ]
            
            if any(indicator in question_lower for indicator in non_procurement_indicators):
                return False
            
            # Only offer email for procurement-related questions
            procurement_indicators = [
                "procurement", "purchase", "buy", "vendor", "supplier", "contract",
                "requisition", "order", "approval", "policy", "process", "requirement"
            ]
            
            return any(indicator in question_lower for indicator in procurement_indicators)
        
        should_offer_email = should_offer_email_for_no_docs(question)
        
        if should_offer_email:
            fallback_prompt = PromptTemplate(
                template="""You are a helpful procurement assistant. A search was performed for a user's question, but no directly relevant documents were found.
                
                Your task is to inform the user of this limitation in a professional and helpful way, while being proactively helpful. DO NOT invent an answer.
                
                USER'S QUESTION: "{question}"
                
                Compose a response that:
                1.  Acknowledges their question professionally.
                2.  States that you were unable to find specific information in the available documents.
                3.  Mentions that Richard Pallangyo (rapaugustino@gmail.com) is the procurement contact who can help.
                4.  **SUGGEST EMAIL CONTACT**: Suggest they can email Richard directly about their specific question.
                5.  Provide his email address for direct contact.
                
                Response:""",
                input_variables=["question"],
            )
        else:
            fallback_prompt = PromptTemplate(
                template="""You are a helpful procurement assistant. A search was performed for a user's question, but no directly relevant documents were found.
                
                Your task is to inform the user of this limitation in a professional and helpful way. DO NOT invent an answer.
                
                USER'S QUESTION: "{question}"
                
                Compose a response that:
                1.  Acknowledges their question professionally.
                2.  States that you were unable to find specific information in the available documents.
                3.  Mentions that Richard Pallangyo (rapaugustino@gmail.com) is the procurement contact who can help with procurement-related questions.
                4.  Suggests they contact Richard directly if this is procurement-related.
                
                Response:""",
                input_variables=["question"],
            )
        
        fallback_chain = fallback_prompt | self.llm | StrOutputParser()
        generation = fallback_chain.invoke({"question": question})
        return {"generation": generation, "documents": []}
    
    def _update_memory(self, state: RAGGraphState) -> dict:
        """Update conversation memory"""
        memory = state["conversation_memory"]
        question = state["original_question"]
        generation = state["generation"]
        
        memory.setdefault("history", []).append({
            "question": question,
            "answer": generation,
        })
        
        memory["history"] = memory["history"][-5:]
        self.memory_service.save_conversation_memory(memory)
        
        return state